{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vborb\\Desktop\\shareables\\llama_faiss_lucy\\llama_faiss\\notebooks\\faiss_rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 691.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es la inteligencia artificial?\n",
      "Respuesta:  La inteligencia artificial (IA) es el campo de investigación que busca simular y mejorar la capacidad de los sistemas informáticos para resolver problemas complejos, como el reconocimiento de voz, la visión por computadora, el procesamiento de lenguaje natural, entre otros. La IA se basa en algoritmos y modelos que imitan los patrones y comportamientos humanos, permitiendo la interacción con sistemas de apoyo.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Textos de ejemplo para la base de datos\n",
    "documents = [\n",
    "    \"Python es un lenguaje de programación interpretado, interactivo y orientado a objetos.\",\n",
    "    \"La capital de Francia es París, conocida por su arquitectura, cultura y gastronomía.\",\n",
    "    \"La inteligencia artificial se refiere a la simulación de procesos de inteligencia humana por parte de sistemas informáticos.\",\n",
    "    \"El café es una bebida elaborada a partir de granos tostados de la planta de café.\"\n",
    "]\n",
    "\n",
    "# Convertir los documentos a vectores de embeddings\n",
    "doc_vectors = embedding_model.encode(documents).astype('float32')\n",
    "\n",
    "# Crear el índice de FAISS\n",
    "d = doc_vectors.shape[1]  # Dimensionalidad de los vectores\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(doc_vectors)\n",
    "\n",
    "# Función para buscar los documentos más similares usando FAISS\n",
    "def search_documents(query, k=2):\n",
    "    query_vector = embedding_model.encode([query]).astype('float32')\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    return [documents[idx] for idx in indices[0]]\n",
    "\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "# Cargar el modelo y el tokenizer de LLaMA\n",
    "llama_model = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "model = CTransformers(\n",
    "            model = llama_model,\n",
    "            model_type=\"llama\",\n",
    "            max_new_tokens = 512,\n",
    "            temperature = 0.5\n",
    "        )\n",
    "\n",
    "\n",
    "# Crear una cadena de LLM utilizando LangChain\n",
    "prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], \n",
    "                                 template=\"Usa la siguiente información para responder la pregunta.\\n\\nContexto: {context}\\n\\nPregunta: {question}\\n\\nRespuesta:\")\n",
    "\n",
    "def generate_response(context, question):\n",
    "    input_text = prompt_template.format(context=context, question=question)\n",
    "    response = model.invoke(input_text)\n",
    "    return response\n",
    "\n",
    "# Función de generación aumentada por recuperación\n",
    "def rag_generate(question):\n",
    "    context = search_documents(question)\n",
    "    context_str = \" \".join(context)\n",
    "    response = generate_response(context_str, question)\n",
    "    return response\n",
    "\n",
    "# Ejemplo de uso\n",
    "question = \"¿Qué es la inteligencia artificial?\"\n",
    "response = rag_generate(question)\n",
    "print(\"Pregunta:\", question)\n",
    "print(\"Respuesta:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usa la siguiente información para responder la pregunta.\n",
      "\n",
      "Contexto: contexto\n",
      "\n",
      "Pregunta: pregunta\n",
      "\n",
      "Respuesta:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(context=\"contexto\", question=\"pregunta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vborb\\Desktop\\shareables\\llama_faiss_lucy\\llama_faiss\\notebooks\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\vborb\\Desktop\\shareables\\llama_faiss_lucy\\llama_faiss\\notebooks\\.venv\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\vborb\\Desktop\\shareables\\llama_faiss_lucy\\llama_faiss\\notebooks\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vborb\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\vborb\\Desktop\\shareables\\llama_faiss_lucy\\llama_faiss\\notebooks\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: Para hacer una ensalada César, mezcla lechuga romana con trozos de pan tostado, queso parmesano, y aderezo César.\n",
      "Textos similares:\n",
      "  - La receta del guacamole es simple: machaca los aguacates y mézclalos con tomate, cebolla, cilantro, y un poco de limón. (Distancia: 19.33415985107422)\n",
      "  - El bosque estaba tranquilo, con árboles altos que se mecían suavemente con el viento. Un río serpenteaba a través del paisaje, reflejando el cielo azul. (Distancia: 21.67862319946289)\n",
      "\n",
      "Consulta: La playa estaba desierta, con arena blanca y olas suaves que rompían en la orilla bajo el cielo despejado.\n",
      "Textos similares:\n",
      "  - El bosque estaba tranquilo, con árboles altos que se mecían suavemente con el viento. Un río serpenteaba a través del paisaje, reflejando el cielo azul. (Distancia: 18.767475128173828)\n",
      "  - La receta del guacamole es simple: machaca los aguacates y mézclalos con tomate, cebolla, cilantro, y un poco de limón. (Distancia: 23.12240219116211)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "embeddings_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Textos intuitivos de ejemplo\n",
    "database_texts = [\n",
    "    \"Para hacer una tortilla de patatas, primero pela y corta las patatas en rodajas finas. Luego, fríelas en aceite caliente hasta que estén doradas.\",\n",
    "    \"La receta del guacamole es simple: machaca los aguacates y mézclalos con tomate, cebolla, cilantro, y un poco de limón.\",\n",
    "    \"El bosque estaba tranquilo, con árboles altos que se mecían suavemente con el viento. Un río serpenteaba a través del paisaje, reflejando el cielo azul.\",\n",
    "    \"Desde la cima de la montaña, la vista era impresionante. Podías ver kilómetros de bosque, con el sol poniéndose en el horizonte.\"\n",
    "]\n",
    "\n",
    "query_texts = [\n",
    "    \"Para hacer una ensalada César, mezcla lechuga romana con trozos de pan tostado, queso parmesano, y aderezo César.\",\n",
    "    \"La playa estaba desierta, con arena blanca y olas suaves que rompían en la orilla bajo el cielo despejado.\"\n",
    "]\n",
    "\n",
    "# Convertir los textos en embeddings\n",
    "database_vectors = embeddings_model.encode(database_texts)\n",
    "query_vectors = embeddings_model.encode(query_texts)\n",
    "\n",
    "# Convertir los embeddings a tipo float32 (requerido por FAISS)\n",
    "database_vectors = np.array(database_vectors).astype('float32')\n",
    "query_vectors = np.array(query_vectors).astype('float32')\n",
    "\n",
    "# Crear el índice de FAISS\n",
    "d = database_vectors.shape[1]  # Dimensionalidad de los vectores\n",
    "index = faiss.IndexFlatL2(d)  # Usar distancia euclidiana\n",
    "\n",
    "# Agregar los vectores de la base de datos al índice\n",
    "index.add(database_vectors)\n",
    "\n",
    "# Realizar búsqueda de similitud\n",
    "k = 2  # Número de vecinos más cercanos a recuperar\n",
    "distances, indices = index.search(query_vectors, k)\n",
    "\n",
    "# Mostrar resultados\n",
    "for i, query in enumerate(query_texts):\n",
    "    print(f\"Consulta: {query}\")\n",
    "    print(\"Textos similares:\")\n",
    "    for j in range(k):\n",
    "        print(f\"  - {database_texts[indices[i, j]]} (Distancia: {distances[i, j]})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de database_vectors: 4\n",
      "Longitud de query_vectors: 2\n",
      "Longitud del primer elemento de database_vectors: 384\n",
      "Longitud del primer elemento de query_vectors: 384\n"
     ]
    }
   ],
   "source": [
    "print(\"Longitud de database_vectors:\", len(database_vectors))\n",
    "print(\"Longitud de query_vectors:\", len(query_vectors))\n",
    "print(\"Longitud del primer elemento de database_vectors:\", len(database_vectors[0]))\n",
    "print(\"Longitud del primer elemento de query_vectors:\", len(query_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para hacer una ensalada César, mezcla lechuga roma...: [-0.01954905  0.17426275  0.0006641 ..]\n",
      "La receta del guacamole es simple: machaca los agu...: [-0.07286499  0.17364433 -0.14115788..]\n"
     ]
    }
   ],
   "source": [
    "print(f'{query_texts[0][:50]}...:', f'{f'{query_vectors[0][:3]}'[:-1]}..]')\n",
    "print(f'{database_texts[1][:50]}...:', f'{f'{database_vectors[1][:3]}'[:-1]}..]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
